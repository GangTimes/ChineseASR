{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    data_path='data/zh.tsv'\n",
    "    dict_path='data/dict.txt'\n",
    "    model_path='logs/model'\n",
    "    new_dict='data/ndict.txt'\n",
    "    embed_size = 300\n",
    "    lr = 0.0003\n",
    "    is_training = True\n",
    "    epochs = 25\n",
    "    batch_size = 4\n",
    "    num_heads = 8\n",
    "    num_blocks = 6\n",
    "    input_vocab_size = 50\n",
    "    label_vocab_size = 50\n",
    "    # embedding size\n",
    "    max_length = 100\n",
    "    hidden_units = 512\n",
    "    dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(dcit_path=Config.dict_path):\n",
    "    \"\"\"\n",
    "    根据路径dict_path读取文本和英文字典\n",
    "    return: pny2idx idx2pny hanzi2idx idx2hanzi\n",
    "    \"\"\"\n",
    "    pnys=['<PAD>']\n",
    "    hanzis=['<PAD>']\n",
    "    \n",
    "    with open(dcit_path,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            pny,hanzi=line.strip('\\n').split('\\t')\n",
    "            pnys.append(pny)\n",
    "            hanzis+=list(hanzi)\n",
    "    hanzis=list(set(hanzis))\n",
    "    pny2idx={pny:idx for idx,pny in enumerate(pnys)}\n",
    "    idx2pny={idx:pny for idx,pny in enumerate(pnys)}\n",
    "    hanzi2idx={hanzi:idx for idx,hanzi in enumerate(hanzis)}\n",
    "    idx2hanzi={idx:hanzi for idx,hanzi in enumerate(hanzis)}\n",
    "    Config.input_vocab_size=len(pny2idx)\n",
    "    Config.label_vocab_size=len(hanzi2idx)\n",
    "    return pny2idx,idx2pny,hanzi2idx,idx2hanzi\n",
    "a,b,c,d=read_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    根据路径data_path读取中文文本到英文文本的对应关系  \n",
    "    return: inputs->拼音->[[一句话的拼音列表],[]]  lables->汉字->[[一句话的汉字列表],[]]\n",
    "    \"\"\"\n",
    "    inputs=[]\n",
    "    labels=[]\n",
    "    hanzid=[]\n",
    "    pnysd=[]\n",
    "    phdict={}\n",
    "    with open(Config.dict_path,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            pny,hanzi=line.strip('\\n').split('\\t')\n",
    "            phdict[pny]=list(hanzi)\n",
    "            \n",
    "    with open(Config.data_path,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            key,pny,hanzi=line.strip('\\n').strip().split('\\t')\n",
    "            pnys=pny.strip().split(' ')\n",
    "            hanzis=hanzi.strip().split(' ')\n",
    "            \n",
    "            inputs.append(pnys)\n",
    "            labels.append(hanzis)\n",
    "            assert len(pnys)==len(hanzis)\n",
    "            if '' in pnys:\n",
    "                \n",
    "                hanzis.remove('')\n",
    "                pnys.remove('')\n",
    "            for ipny,ihanzi in zip(pnys,hanzis):\n",
    "                if ipny in phdict.keys():\n",
    "                    if ihanzi not in phdict[ipny]:\n",
    "                        phdict[ipny].append(ihanzi)\n",
    "                else:\n",
    "                    phdict[ipny]=ihanzi\n",
    "    with open(Config.new_dict,'w',encoding='utf-8') as file:\n",
    "        for key,value in phdict.items():\n",
    "            file.write(key+'\\t'+' '.join(value)+'\\n')\n",
    "        \n",
    "    pny2idx,idx2pny,hanzi2idx,idx2hanzi=read_dict(Config.new_dict)\n",
    "    input_num = [[pny2idx[pny] for pny in line ] for line in inputs]\n",
    "    label_num = [[hanzi2idx[han] for han in line] for line in labels]\n",
    "\n",
    "    return input_num,label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(input_data, label_data, batch_size):\n",
    "    batch_num = len(input_data) // batch_size\n",
    "    for k in range(batch_num):\n",
    "        begin = k * batch_size\n",
    "        end = begin + batch_size\n",
    "        input_batch = input_data[begin:end]\n",
    "        label_batch = label_data[begin:end]\n",
    "        max_len = max([len(line) for line in input_batch])\n",
    "        input_batch = np.array([line + [0] * (max_len - len(line)) for line in input_batch])\n",
    "        label_batch = np.array([line + [0] * (max_len - len(line)) for line in label_batch])\n",
    "        yield input_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(inputs, \n",
    "              vocab_size, \n",
    "              num_units, \n",
    "              zero_pad=True, \n",
    "              scale=True,\n",
    "              scope=\"embedding\", \n",
    "              reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "\n",
    "    For example,\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]\n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(emb,\n",
    "                        queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(emb, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "   \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "         \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(emb, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training)\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self, is_training=True):\n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = Config.is_training\n",
    "        self.hidden_units = Config.hidden_units\n",
    "        self.input_vocab_size = Config.input_vocab_size\n",
    "        self.label_vocab_size = Config.label_vocab_size\n",
    "        self.num_heads = Config.num_heads\n",
    "        self.num_blocks = Config.num_blocks\n",
    "        self.max_length = Config.max_length\n",
    "        self.lr = Config.lr\n",
    "        self.dropout_rate = Config.dropout_rate\n",
    "        \n",
    "        # input\n",
    "        self.x = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        # embedding\n",
    "        self.emb = embedding(self.x, vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True, scope=\"enc_embed\")\n",
    "        self.enc = self.emb + embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                      vocab_size=self.max_length,num_units=self.hidden_units, zero_pad=False, scale=False,scope=\"enc_pe\")\n",
    "        ## Dropout\n",
    "        self.enc = tf.layers.dropout(self.enc, \n",
    "                                    rate=self.dropout_rate, \n",
    "                                    training=self.is_training)\n",
    "    \n",
    "                \n",
    "        ## Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                ### Multihead Attention\n",
    "                self.enc = multihead_attention(emb = self.emb,\n",
    "                                               queries=self.enc, \n",
    "                                                keys=self.enc, \n",
    "                                                num_units=self.hidden_units, \n",
    "                                                num_heads=self.num_heads, \n",
    "                                                dropout_rate=self.dropout_rate,\n",
    "                                                is_training=self.is_training,\n",
    "                                                causality=False)\n",
    "                        \n",
    "        ### Feed Forward\n",
    "        self.outputs = feedforward(self.enc, num_units=[4*self.hidden_units, self.hidden_units])\n",
    "            \n",
    "                \n",
    "        # Final linear projection\n",
    "        self.logits = tf.layers.dense(self.outputs, self.label_vocab_size)\n",
    "        self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1))\n",
    "        self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "        tf.summary.scalar('acc', self.acc)\n",
    "                \n",
    "        if is_training:  \n",
    "            # Loss\n",
    "            self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=self.label_vocab_size))\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "            self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "               \n",
    "            # Training Scheme\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "            self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "                   \n",
    "            # Summary \n",
    "            tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "            self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    input_num,label_num=read_data()\n",
    "    batch_size=Config.batch_size\n",
    "    g = Graph()\n",
    "\n",
    "    saver =tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        merged = tf.summary.merge_all()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if os.path.exists('logs/model.meta'):\n",
    "            saver.restore(sess, 'logs/model')\n",
    "        writer = tf.summary.FileWriter('tensorboard/lm', tf.get_default_graph())\n",
    "        for k in range(Config.epochs):\n",
    "            total_loss = 0\n",
    "            batch_num = len(input_num) // batch_size\n",
    "            batch = get_batch(input_num, label_num, batch_size)\n",
    "            for i in range(batch_num):\n",
    "                input_batch, label_batch = next(batch)\n",
    "                feed = {g.x: input_batch, g.y: label_batch}\n",
    "                cost,_ = sess.run([g.mean_loss,g.train_op], feed_dict=feed)\n",
    "                total_loss += cost\n",
    "                if (k * batch_num + i) % 10 == 0:\n",
    "                    rs=sess.run(merged, feed_dict=feed)\n",
    "                    writer.add_summary(rs, k * batch_num + i)\n",
    "            if (k+1) % 5 == 0:\n",
    "                print('epochs', k+1, ': average loss = ', total_loss/batch_num)\n",
    "        saver.save(sess, 'logs/model')\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    Config.is_training = False\n",
    "\n",
    "    g = Graph()\n",
    "\n",
    "    saver =tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, 'logs/model')\n",
    "        while True:\n",
    "            line = input('输入测试拼音: ')\n",
    "            if line == 'exit': break\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            x = np.array([pny2id.index(pny) for pny in line])\n",
    "            x = x.reshape(1, -1)\n",
    "            preds = sess.run(g.preds, {g.x: x})\n",
    "            got = ''.join(han2id[idx] for idx in preds[0])\n",
    "            print(got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
