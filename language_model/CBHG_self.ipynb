{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建基于CBHG的拼音到汉字语言模型¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    data_path='data/zh.tsv'\n",
    "    dict_path='data/dict.txt'\n",
    "    model_path='logs/model'\n",
    "    new_dict='data/ndict.txt'\n",
    "    embed_size = 300\n",
    "    num_highwaynet_blocks = 4\n",
    "    encoder_num_banks = 8\n",
    "    lr = 0.001\n",
    "    is_training = True\n",
    "    epochs = 25\n",
    "    batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(dcit_path=Config.dict_path):\n",
    "    \"\"\"\n",
    "    根据路径dict_path读取文本和英文字典\n",
    "    return: pny2idx idx2pny hanzi2idx idx2hanzi\n",
    "    \"\"\"\n",
    "    pnys=['<PAD>']\n",
    "    hanzis=['<PAD>']\n",
    "    \n",
    "    with open(dcit_path,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            pny,hanzi=line.strip('\\n').split('\\t')\n",
    "            pnys.append(pny)\n",
    "            hanzis+=list(hanzi)\n",
    "    hanzis=list(set(hanzis))\n",
    "    pny2idx={pny:idx for idx,pny in enumerate(pnys)}\n",
    "    idx2pny={idx:pny for idx,pny in enumerate(pnys)}\n",
    "    hanzi2idx={hanzi:idx for idx,hanzi in enumerate(hanzis)}\n",
    "    idx2hanzi={idx:hanzi for idx,hanzi in enumerate(hanzis)}\n",
    "    Config.pny_size=len(pny2idx)\n",
    "    Config.hanzi_size=len(hanzi2idx)\n",
    "    return pny2idx,idx2pny,hanzi2idx,idx2hanzi\n",
    "a,b,c,d=read_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    根据路径data_path读取中文文本到英文文本的对应关系  \n",
    "    return: inputs->拼音->[[一句话的拼音列表],[]]  lables->汉字->[[一句话的汉字列表],[]]\n",
    "    \"\"\"\n",
    "    inputs=[]\n",
    "    labels=[]\n",
    "    hanzid=[]\n",
    "    pnysd=[]\n",
    "    phdict={}\n",
    "    with open(Config.dict_path,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            pny,hanzi=line.strip('\\n').split('\\t')\n",
    "            phdict[pny]=list(hanzi)\n",
    "            \n",
    "    with open(Config.data_path,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            key,pny,hanzi=line.strip('\\n').strip().split('\\t')\n",
    "            pnys=pny.strip().split(' ')\n",
    "            hanzis=hanzi.strip().split(' ')\n",
    "            \n",
    "            inputs.append(pnys)\n",
    "            labels.append(hanzis)\n",
    "            assert len(pnys)==len(hanzis)\n",
    "            if '' in pnys:\n",
    "                \n",
    "                hanzis.remove('')\n",
    "                pnys.remove('')\n",
    "            for ipny,ihanzi in zip(pnys,hanzis):\n",
    "                if ipny in phdict.keys():\n",
    "                    if ihanzi not in phdict[ipny]:\n",
    "                        phdict[ipny].append(ihanzi)\n",
    "                else:\n",
    "                    phdict[ipny]=ihanzi\n",
    "    with open(Config.new_dict,'w',encoding='utf-8') as file:\n",
    "        for key,value in phdict.items():\n",
    "            file.write(key+'\\t'+' '.join(value)+'\\n')\n",
    "        \n",
    "    pny2idx,idx2pny,hanzi2idx,idx2hanzi=read_dict(Config.new_dict)\n",
    "    input_num = [[pny2idx[pny] for pny in line ] for line in inputs]\n",
    "    label_num = [[hanzi2idx[han] for han in line] for line in labels]\n",
    "\n",
    "    return input_num,label_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(inputs,labels):\n",
    "    batch_size=Config.batch_size\n",
    "    batch_num = len(inputs) // batch_size\n",
    "    for k in range(batch_num):\n",
    "        begin = k * batch_size\n",
    "        end = begin + batch_size\n",
    "        input_batch = inputs[begin:end]\n",
    "        label_batch = labels[begin:end]\n",
    "        max_len = max([len(line) for line in input_batch])\n",
    "        input_batch = np.array([line + [0] * (max_len - len(line)) for line in input_batch])\n",
    "        label_batch = np.array([line + [0] * (max_len - len(line)) for line in label_batch])\n",
    "        yield input_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(inputs, vocab_size, num_units, zero_pad=True, scope=\"embedding\", reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "    return tf.nn.embedding_lookup(lookup_table, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prenet(inputs, num_units=None, is_training=True, scope=\"prenet\", reuse=None, dropout_rate=0.2):\n",
    "    '''\n",
    "    inputs: batch_size*length*embed_size\n",
    "    return:batch_size*length*num_units/2\n",
    "    '''\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = tf.layers.dense(inputs, units=num_units[0], activation=tf.nn.relu, name=\"dense1\")\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training, name=\"dropout1\")\n",
    "        outputs = tf.layers.dense(outputs, units=num_units[1], activation=tf.nn.relu, name=\"dense2\")\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training, name=\"dropout2\")\n",
    "    return outputs  # (N, ..., num_units[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inputs,filters=None, size=1,rate=1, padding=\"SAME\",use_bias=False,activation_fn=None, scope=\"conv1d\", reuse=None):\n",
    "    '''\n",
    "    Args:\n",
    "      inputs: A 3-D tensor with shape of [batch, time, depth].\n",
    "      filters: An int. Number of outputs (=activation maps)\n",
    "      size: An int. Filter size.\n",
    "      rate: An int. Dilation rate.\n",
    "      padding: Either `same` or `valid` or `causal` (case-insensitive).\n",
    "      use_bias: A boolean.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A masked tensor of the same shape and dtypes as `inputs`.\n",
    "    '''    \n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == \"causal\":\n",
    "            # pre-padding for causality\n",
    "            pad_len = (size - 1) * rate  # padding size\n",
    "            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n",
    "            padding = \"valid\"\n",
    "\n",
    "        if filters is None:\n",
    "            filters = inputs.get_shape().as_list[-1]\n",
    "\n",
    "        params = {\"inputs\": inputs, \"filters\": filters, \"kernel_size\": size,\n",
    "                  \"dilation_rate\": rate, \"padding\": padding, \"activation\": activation_fn,\n",
    "                  \"use_bias\": use_bias, \"reuse\": reuse}\n",
    "\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_banks(inputs, num_units=None, K=16, is_training=True, scope=\"conv1d_banks\", reuse=None):\n",
    "    '''Applies a series of conv1d separately.\n",
    "\n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C]\n",
    "      K: An int. The size of conv1d banks. That is,\n",
    "        The `inputs` are convolved with K filters: 1, 2, ..., K.\n",
    "      is_training: A boolean. This is passed to an argument of `batch_normalize`.\n",
    "\n",
    "    Returns:\n",
    "      A 3d tensor with shape of [N, T, K*Hp.embed_size//2].\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = conv1d(inputs, num_units // 2, 1)  # k=1\n",
    "        for k in range(2, K + 1):  # k = 2...K\n",
    "            with tf.variable_scope(\"num_{}\".format(k)):\n",
    "                output = conv1d(inputs, num_units, k)\n",
    "                outputs = tf.concat((outputs, output), -1)\n",
    "        outputs = normalize(outputs, is_training=is_training,\n",
    "                            activation_fn=tf.nn.relu)\n",
    "    return outputs  # (N, T, Hp.embed_size//2*K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_banks(inputs, num_units=None, K=16, is_training=True, scope=\"conv1d_banks\", reuse=None):\n",
    "    '''Applies a series of conv1d separately.\n",
    "\n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C]\n",
    "      K: An int. The size of conv1d banks. That is,\n",
    "        The `inputs` are convolved with K filters: 1, 2, ..., K.\n",
    "      is_training: A boolean. This is passed to an argument of `batch_normalize`.\n",
    "\n",
    "    Returns:\n",
    "      A 3d tensor with shape of [N, T, K*Hp.embed_size//2].\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = conv1d(inputs, num_units // 2, 1)  # k=1\n",
    "        for k in range(2, K + 1):  # k = 2...K\n",
    "            with tf.variable_scope(\"num_{}\".format(k)):\n",
    "                output = conv1d(inputs, num_units, k)\n",
    "                outputs = tf.concat((outputs, output), -1)\n",
    "        outputs = normalize(outputs, is_training=is_training,\n",
    "                            activation_fn=tf.nn.relu)\n",
    "    return outputs  # (N, T, Hp.embed_size//2*K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(inputs, num_units=None, bidirection=False, seqlen=None, scope=\"gru\", reuse=None):\n",
    "    '''Applies a GRU.\n",
    "\n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: An int. The number of hidden units.\n",
    "      bidirection: A boolean. If True, bidirectional results\n",
    "        are concatenated.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      If bidirection is True, a 3d tensor with shape of [N, T, 2*num_units],\n",
    "        otherwise [N, T, num_units].\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if num_units is None:\n",
    "            num_units = inputs.get_shape().as_list[-1]\n",
    "\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units)\n",
    "        if bidirection:\n",
    "            cell_bw = tf.contrib.rnn.GRUCell(num_units)\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, inputs,\n",
    "                                                         sequence_length=seqlen,\n",
    "                                                         dtype=tf.float32)\n",
    "            return tf.concat(outputs, 2)\n",
    "        else:\n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell, inputs,\n",
    "                                           sequence_length=seqlen,\n",
    "                                           dtype=tf.float32)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highwaynet(inputs, num_units=None, scope=\"highwaynet\", reuse=None):\n",
    "    '''Highway networks, see https://arxiv.org/abs/1505.00387\n",
    "    Args:\n",
    "      inputs: A 3D tensor of shape [N, T, W].\n",
    "      num_units: An int or `None`. Specifies the number of units in the highway layer\n",
    "             or uses the input size if `None`.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A 3D tensor of shape [N, T, W].\n",
    "    '''\n",
    "    if not num_units:\n",
    "        num_units = inputs.get_shape()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        H = tf.layers.dense(inputs, units=num_units, activation=tf.nn.relu, name=\"dense1\")\n",
    "        T = tf.layers.dense(inputs, units=num_units, activation=tf.nn.sigmoid,\n",
    "                            bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\")\n",
    "        C = 1. - T\n",
    "        outputs = H * T + inputs * C\n",
    "\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs,\n",
    "              decay=.99,\n",
    "              epsilon=1e-8,\n",
    "              is_training=True,\n",
    "              activation_fn=None,\n",
    "              reuse=None,\n",
    "              scope=\"normalize\"):\n",
    "    '''Applies {batch|layer} normalization.\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`. If type is `bn`, the normalization is over all but\n",
    "        the last dimension. Or if type is `ln`, the normalization is over\n",
    "        the last dimension. Note that this is different from the native\n",
    "        `tf.contrib.layers.batch_norm`. For this I recommend you change\n",
    "        a line in ``tensorflow/contrib/layers/python/layers/layer.py`\n",
    "        as follows.\n",
    "        Before: mean, variance = nn.moments(inputs, axis, keep_dims=True)\n",
    "        After: mean, variance = nn.moments(inputs, [-1], keep_dims=True)\n",
    "      type: A string. Either \"bn\" or \"ln\".\n",
    "      decay: Decay for the moving average. Reasonable values for `decay` are close\n",
    "        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n",
    "        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n",
    "        reasonably good training performance but poor validation and/or test\n",
    "        performance.\n",
    "      is_training: Whether or not the layer is in training mode. W\n",
    "      activation_fn: Activation function.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "\n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    inputs_shape = inputs.get_shape()\n",
    "    inputs_rank = inputs_shape.ndims\n",
    "\n",
    "    # use fused batch norm if inputs_rank in [2, 3, 4] as it is much faster.\n",
    "    # pay attention to the fact that fused_batch_norm requires shape to be rank 4 of NHWC.\n",
    "    inputs = tf.expand_dims(inputs, axis=1)\n",
    "    outputs = tf.contrib.layers.batch_norm(inputs=inputs,\n",
    "                                            decay=decay,\n",
    "                                            center=True,\n",
    "                                            scale=True,\n",
    "                                            updates_collections=None,\n",
    "                                            is_training=is_training,\n",
    "                                            scope=scope,\n",
    "                                            zero_debias_moving_mean=True,\n",
    "                                            fused=True,\n",
    "                                            reuse=reuse)\n",
    "    outputs = tf.squeeze(outputs, axis=1)\n",
    "\n",
    "    if activation_fn:\n",
    "        outputs = activation_fn(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    '''Builds a model graph'''\n",
    "\n",
    "    def __init__(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.pny_size = Config.pny_size\n",
    "        self.han_size = Config.hanzi_size\n",
    "        self.embed_size = Config.embed_size\n",
    "        self.is_training = Config.is_training\n",
    "        self.num_highwaynet_blocks = Config.num_highwaynet_blocks\n",
    "        self.encoder_num_banks = Config.encoder_num_banks\n",
    "        self.lr = Config.lr\n",
    "        \n",
    "        self.x = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        \n",
    "        # Character Embedding for x\n",
    "\n",
    "        enc = embed(self.x, self.pny_size, self.embed_size, scope=\"emb_x\")\n",
    "        # Encoder pre-net\n",
    "        prenet_out = prenet(enc,\n",
    "                            num_units=[self.embed_size, self.embed_size // 2],\n",
    "                            is_training=self.is_training)  # (N, T, E/2)\n",
    "\n",
    "        # Encoder CBHG\n",
    "        ## Conv1D bank\n",
    "        enc = conv1d_banks(prenet_out,\n",
    "                            K=self.encoder_num_banks,\n",
    "                            num_units=self.embed_size // 2,\n",
    "                            is_training=self.is_training)  # (N, T, K * E / 2)\n",
    "\n",
    "        ## Max pooling\n",
    "        enc = tf.layers.max_pooling1d(enc, 2, 1, padding=\"same\")  # (N, T, K * E / 2)\n",
    "\n",
    "        ## Conv1D projections\n",
    "        enc = conv1d(enc, self.embed_size // 2, 5, scope=\"conv1d_1\")  # (N, T, E/2)\n",
    "        enc = normalize(enc, is_training=self.is_training,\n",
    "                            activation_fn=tf.nn.relu, scope=\"norm1\")\n",
    "        enc = conv1d(enc, self.embed_size // 2, 5, scope=\"conv1d_2\")  # (N, T, E/2)\n",
    "        enc = normalize(enc, is_training=self.is_training,\n",
    "                            activation_fn=None, scope=\"norm2\")\n",
    "        enc += prenet_out  # (N, T, E/2) # residual connections\n",
    "\n",
    "        ## Highway Nets\n",
    "        for i in range(self.num_highwaynet_blocks):\n",
    "            enc = highwaynet(enc, num_units=self.embed_size // 2,\n",
    "                                scope='highwaynet_{}'.format(i))  # (N, T, E/2)\n",
    "\n",
    "        ## Bidirectional GRU\n",
    "        enc = gru(enc, self.embed_size // 2, True, scope=\"gru1\")  # (N, T, E)\n",
    "\n",
    "        ## Readout\n",
    "        self.outputs = tf.layers.dense(enc, self.han_size, use_bias=False)\n",
    "        self.preds = tf.to_int32(tf.argmax(self.outputs, axis=-1))\n",
    "\n",
    "        if self.is_training:\n",
    "            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.outputs)\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.y, tf.zeros_like(self.y)))  # masking\n",
    "            self.hits = tf.to_float(tf.equal(self.preds, self.y)) * self.istarget\n",
    "            self.acc = tf.reduce_sum(self.hits) / tf.reduce_sum(self.istarget)\n",
    "            self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / tf.reduce_sum(self.istarget)\n",
    "\n",
    "            # Training Scheme\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "\n",
    "            # Summary\n",
    "            tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "            tf.summary.scalar('acc', self.acc)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    inputs,labels=read_data()\n",
    "    g = Graph()\n",
    "\n",
    "    saver =tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        merged = tf.summary.merge_all()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if os.path.exists('logs/model.meta'):\n",
    "            saver.restore(sess, 'logs/model')\n",
    "        writer = tf.summary.FileWriter('tensorboard/lm', tf.get_default_graph())\n",
    "        \n",
    "        batch_num = len(inputs) // Config.batch_size\n",
    "        for k in range(Config.epochs):\n",
    "            total_loss = 0\n",
    "            batch = get_batch(inputs, labels)\n",
    "            for i in range(batch_num):\n",
    "                input_batch, label_batch = next(batch)\n",
    "                feed = {g.x: input_batch, g.y: label_batch}\n",
    "                cost,_ = sess.run([g.mean_loss,g.train_op], feed_dict=feed)\n",
    "                total_loss += cost\n",
    "                if (k * batch_num + i) % 10 == 0:\n",
    "                    rs=sess.run(merged, feed_dict=feed)\n",
    "                    writer.add_summary(rs, k * batch_num + i)\n",
    "            if (k+1) % 5 == 0:\n",
    "                print('epochs', k+1, ': average loss = ', total_loss/batch_num)\n",
    "        saver.save(sess, 'logs/model')\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    Config.is_training=False\n",
    "    g = Graph()\n",
    "\n",
    "    saver =tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, Config.model_path)\n",
    "        while True:\n",
    "            line = input('输入测试拼音: ')\n",
    "            if line == 'exit': break\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            x = np.array([pny2id.index(pny) for pny in line])\n",
    "            x = x.reshape(1, -1)\n",
    "            preds = sess.run(g.preds, {g.x: x})\n",
    "            got = ''.join(han2id[idx] for idx in preds[0])\n",
    "            print(got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
